{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Curso de Python para Economistas\n",
    "## Bienvenidos a la clase 5\n",
    "\n",
    "### Anuncios y temario para hoy\n",
    "\n",
    "- Pull de GitHub\n",
    "- Cuestionario dificultad tarea\n",
    "- Respuestas del cuestionario de los videos\n",
    "- Dudas de la teoría\n",
    "- Ejercicio práctico: Analisis de sentimiento de tweets de Joe Biden pre y post elecciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuestionario dificultad Tarea\n",
    "https://forms.gle/yCLHV93TFNLd9Ug86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respuestas a las preguntas de los videos:\n",
    "https://docs.google.com/forms/d/e/1FAIpQLSfb7zlEHh3gllD3iUUWwPCMwyQPd0Z04F3XpArSKQzPkYvGHA/viewform?usp=pp_url&entry.1318485631=Respuesta+Satisfactoria+(Successful+responses)&entry.330305262=Una+celda+de+una+tabla&entry.871102831=Tabula&entry.871102831=PyPDF2&entry.871102831=Minecart&entry.2076477619=La+puedo+trabajar+desde+python+con+el+paquete+psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis de sentimiento de tweets de Joe Biden pre y post elecciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importamos los paquetes a utilizar\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tweepy\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "!{sys.executable} -m pip install Pillow\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "from datetime import datetime\n",
    "#Si alguien quiere trabajar con alguna cuenta en español podrían instalar \n",
    "#sentiment_analysis_spanish y utilizarlo en reemplado de TextBlob:\n",
    "#!{sys.executable} -m pip install sentiment_analysis_spanish\n",
    "#from sentiment_analysis_spanish import sentiment_analysis\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "# Si trabajan en español pueden crearse una lista de stopwords con las de este link:\n",
    "# https://github.com/xiamx/node-nltk-stopwords/blob/master/data/stopwords/spanish\n",
    "# También usaremos string.punctuation. Si trabajan en español podrian agregarle  \n",
    "# ¿ y ¡ . En ingles incluye lo siguiente: !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crear con algun editor de texto (ej. Sublime) un archivo llamado `twitter_keys.txt` dentro de la carpeta `clase5` y guardar las 4 claves, una por línea, en el siguiente orden:\n",
    "- API key\n",
    "- API key secret\n",
    "- Access token\n",
    "- Access token secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos variables que contienen nuestas claves de autenticación con la API\n",
    "with open(\"twitter_keys.txt\") as tw_k: \n",
    "    consumer_key = tw_k.readline().strip()\n",
    "    consumer_secret = tw_k.readline().strip()\n",
    "    access_key = tw_k.readline().strip()\n",
    "    access_secret = tw_k.readline().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Le pasamos nuestras credenciales de twitter a tweepy\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este link pueden explorar detalles del metodo user_timeline: https://docs.tweepy.org/en/stable/api.html?highlight=user_timeline#tweepy.API.user_timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_tweets(screen_name, start_date):\n",
    "    '''\n",
    "    Esta funcion recibe el nombre de la persona de quien queremos extraer los \n",
    "    tweets y devuelve una lista con todos los tweets y sus datos\n",
    "    Input: \n",
    "      screen_name (str): el nombre de la persona en twitter\n",
    "    Output:\n",
    "      all_tweets (lista): lista con todos los tweets extraidos\n",
    "    '''\n",
    "    # Solicitamos los 200 tweets mas recientes (200 es el maximo permitido en count)\n",
    "    new_tweets = api.user_timeline(screen_name=screen_name, \n",
    "                                   tweet_mode=\"extended\", count=200)\n",
    "    # Creo una lista para almacenar TODOS los tweets y agrego los recién extraidos\n",
    "    all_tweets = []\n",
    "    all_tweets.extend(new_tweets)\n",
    "    # guardo el id del ultimo tweet extraído \n",
    "    oldest = all_tweets[-1].id \n",
    "    \n",
    "    # Extraigo tweets de a 200 hasta que no haya más\n",
    "    while len(new_tweets) > 0 and all_tweets[-1].created_at > start_date:\n",
    "        # Solicito 200 tweets mas y los agrego a la lista de 'all_tweets'\n",
    "        new_tweets = api.user_timeline(screen_name=screen_name, count=200,\n",
    "                                       tweet_mode=\"extended\", max_id=oldest-1)\n",
    "        all_tweets.extend(new_tweets)\n",
    "        # Actualizo el id del ultimo tweet extraído\n",
    "        oldest = all_tweets[-1].id \n",
    "        print(\"Hasta ahora se han extraido %s tweets\" % len(all_tweets))\n",
    "\n",
    "    return all_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos los tweets desde unos días antes de las elecciones del \n",
    "# 3 de Noviembre de 2020\n",
    "date_before_elections = datetime(2020, 10, 16, 0, 0, 0)\n",
    "all_tweets_biden = get_all_tweets(\"JoeBiden\", date_before_elections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Veamos la lista con los primeros 5 objetos de tweepy\n",
    "all_tweets_biden[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos un solo tweet \n",
    "all_tweets_biden[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos el texto de un solo tweet \n",
    "all_tweets_biden[8].full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1: \n",
    "En la siguiente función construyan un loop que extraiga id_str, created_at, full_text, retweeted,  favorite_count, in_reply_to_screen_name de cada tweet y lo guarde en una lista. Esa lista se debe agregar al final de la lista all_tweets_selection para luego construir un df con la lista de listas (las listas internas serán las filas del df). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweets_text(all_tweets, csv_file=None):\n",
    "    '''\n",
    "    Esta función guarda los tweets en un data frame y si se especifica un \n",
    "    archivo csv tambien se guardaran ahí \n",
    "    Input:\n",
    "        all_tweets (lista): lista con tweets y sus datos\n",
    "        csv_file ('str'): nombre del archivo csv\n",
    "    Output:\n",
    "        df_all_tweets (df): tweets ordenados en una tabla con datos seleccinados\n",
    "    '''\n",
    "    all_tweets_selection = []\n",
    "    \n",
    "    # Su Código acá:\n",
    "    \n",
    "    # Hasta acá su código    \n",
    "    df_all_tweets = pd.DataFrame(all_tweets_selection)\n",
    "    df_all_tweets.columns = ['id_str', 'created_at', 'text', 'retweeted',\n",
    "                            'favorite_count', 'in_reply_to_screen_name']\n",
    "    if csv_file:\n",
    "        df_all_tweets.to_csv(csv_file, index=False, encoding=\"utf-8\")\n",
    "\n",
    "    return df_all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1b: \n",
    "Utilicen la función save_tweets_text para construir una tabla con todos los tweets y guardenlo en una archivo llamado \"tweets.csv\". Por ultimo visualicen algunas filas de la tabla. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_tweets = save_tweets_text(all_tweets_biden, \"tweets.csv\")\n",
    "df_all_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Ejercicio 1b: \n",
    " Hagan print del texto de algunos tweets y luego piensen:\n",
    " \n",
    " 1) Les parece que podríamos hacer un análisis de sentimiento de estos tweets como están?\n",
    " \n",
    " 2) Porque?\n",
    " \n",
    " 3) Que errores creen que podriamos tener al trabajar con el texto tal como lo descargamos de twitter?\n",
    " \n",
    " 4) Que transformaciones le harían al texto antes de meterlo en el análisis (mencionen al menos 5 cosas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emoticones contentos\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "    ])\n",
    " \n",
    "# Emoticones Tristes\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "    ])\n",
    "\n",
    "# Combinamos emoticones contentos y tristes\n",
    "emoticons = emoticons_happy.union(emoticons_sad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # simbolos & pictogramas\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transporte & simbolos mapas\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # banderas (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    '''\n",
    "    Esta función limpia el texto de un tweet. Elimina caracteres especificos que\n",
    "    se utilizan en twitter como los de re-tweets, los links y otros Non-ASCII.\n",
    "    Devuelve el texto \"limpio\".\n",
    "    Input:\n",
    "        tweet (str): Texto del tweet original\n",
    "    Output:\n",
    "        tweet (str): Texto del tweet limpiado\n",
    "    '''   \n",
    "    #Saco los links\n",
    "    tweet = re.sub(r'https.*', '', tweet)\n",
    "    #Elimino caracteres de re-tweets   \n",
    "    tweet = re.sub(r'^RT .*:', '', tweet)\n",
    "    tweet = re.sub(r'@\\S+', '', tweet)\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "    #Reemplazo caracteres non-ASCII con espacio\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "    \n",
    "    return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este es un tweet sucio:\n",
    "df_all_tweets['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este es un tweet limpio:\n",
    "tweet_cleaned = clean_tweet(df_all_tweets.iloc[1]['text'])\n",
    "tweet_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análisis de Sentimiento\n",
    "https://textblob.readthedocs.io/en/dev/api_reference.html#module-textblob.en.sentiments\n",
    "- Polarity: Negative (-1.0) vs. Positive (1.0)\n",
    "- Subjectivity: Objective (0.0) vs. Subjective (1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el sentimiento con el metodo TextBlob\n",
    "blob = TextBlob(tweet_cleaned)\n",
    "Sentiment = blob.sentiment\n",
    "polarity = Sentiment.polarity\n",
    "subjectivity = Sentiment.subjectivity  \n",
    "print(\"La polaridad de este tweet es :\", polarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2:\n",
    "Construyamos un loop que nos permita limpiar todos los strings y concatenarlos. Al tenerlos todos concatenados será más facil utilizarlos en un grafico, como por ejemplo una nube de pablabras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos una imagen de nube de palabras \n",
    "# wordcloud = WordCloud().generate(all_tweets_cleaned)\n",
    "\n",
    "wordcloud = WordCloud().generate(all_tweets) #prueben incluir: max_font_size=40\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(background_color = 'white', stopwords = stopwords.words('english')).generate(all_tweets_cleaned)\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Image.open('upvote.png') as im:\n",
    "    im.rotate(45).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image to np.array\n",
    "mask = np.array(Image.open('upvote.png'))\n",
    "\n",
    "# Generate wordcloud\n",
    "wordcloud = WordCloud(width = 3000, height = 2000, background_color='white', \n",
    "                      collocations=False, stopwords = stopwords.words('english'), \n",
    "                      mask=mask).generate(all_tweets_cleaned)\n",
    "\n",
    "plt.figure(figsize=(40, 30))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota: La imagen del dedo la tomé como idea de este blog https://towardsdatascience.com/simple-wordcloud-in-python-2ae54a9f58e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3:\n",
    "Creen una función que limpie texto en general. Elimina emoticones, emojis, palabras vacías (también llamadas stop words en la libreria de nltk), links, puntuaciones, indicaciones de retweets, etc. Esta función dejara el texto limpio y solo con las palabras que pueden agregar mayor valor para el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, is_tweet=False):\n",
    "    '''\n",
    "    Esta función limpia el texto del tweet. Elimina emoticones, emojis, palabras\n",
    "    vacías (también llamadas stop words en la libreria de nltk), links, puntuaciones\n",
    "    indicaciones de retweets, etc. Para dejar en el texto solo las palabras con \n",
    "    mayor contenido.\n",
    "    Input:\n",
    "        text (str): Texto original\n",
    "        is_tweet (bool): si el texto es un tweet este parametro debe setearse a \n",
    "                         true para que también se limpie el texto con los \n",
    "                         caracteres más especificos de twitter (usa clean_tweet) \n",
    "    Output:\n",
    "        text (str): Texto limpiado\n",
    "    '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_tweets['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(df_all_tweets['text'][1], is_tweet=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
